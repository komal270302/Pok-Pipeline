# PokÃ©Pipeline: A PokÃ©mon Data ETL Pipeline
## Project Overview
PokÃ©Pipeline is an end-to-end ETL (Extract, Transform, Load) pipeline that ingests data from the public PokeAPI, transforms JSON responses into a relational structure, and loads it into a SQL Server database.
This project demonstrates practical data engineering skills in API integration, ETL design, SQL schema modeling, and containerized deployment with Docker.

The project demonstrates :
1. Extraction: Collects detailed PokÃ©mon data (types, abilities, stats, etc.) from the PokÃ©API.
2. Transformation: Cleans and maps JSON responses into a normalized relational schema.
3. Loading: Inserts structured data into SQL Server tables using SQLAlchemy.
4. Containerized: Runs seamlessly with Docker Compose (SQL Server + ETL service).
5. Reusable: Modular design with separate scripts for fetching, transforming, and loading.

This solution is portable and reproducible .It can be run either locally or in a fully containerized environment.

## Tech Stack
- Python 3.11 â€“ ETL scripting
- SQLAlchemy â€“ ORM & database interaction
- SQL Server â€“ Relational database
- Docker & Docker Compose â€“ Container orchestration
- PokeAPI â€“ Data source (public REST API)

## Setup & Running Instructions
1. Clone Repository : 
- git clone https://github.com/komal270302/Pok-Pipeline.git
- cd Pok-Pipeline

2. Environment Variables : Review and update .env with your SQL Server details. The default uses SQL authentication: DB_SERVER=sqlserver DB_NAME=pokemon_db DB_USER=Komal DB_PASSWORD=Komal@123 DB_TRUSTED=NO. For Windows Authentication (commented out), set DB_TRUSTED=YES and remove DB_USER/DB_PASSWORD.
Note: The SQL Server container uses SA_PASSWORD: "Komal@123" (update in docker-compose.yml if changing .env).

3. Run with Docker (recommended ðŸš€)
Build and start services (SQL Server + ETL): docker-compose up --build. This builds the This builds the ETL image from Dockerfile, starts the SQL Server, and runs load_pokemon.py automatically.

4. Run Locally (without Docker) Install dependencies: pip install requests pandas sqlalchemy pyodbc python-dotenv. Ensure SQL Server is running locally (e.g., via SQL Server Express). Run: python load_pokemon.py.

## Database Schema
This design avoids duplication and allows efficient querying. The pipeline normalizes PokÃ©mon data into the following relational tables:
- pokemon â€“ core PokÃ©mon attributes (id, name, height, weight, base_experience)
- type â€“ unique list of PokÃ©mon types
- ability â€“ unique list of abilities
- pokemon_type â€“ mapping between PokÃ©mon and their types
- pokemon_ability â€“ mapping between PokÃ©mon and their abilities
- pokemon_stats â€“ key-value stats (e.g., attack, defense, speed)

## Design Choices
1. ETL Structure:
   - fetch_pokemon.py handles API extraction & raw structuring fetch_pokemon
   - load_pokemon.py manages transformation + SQL inserts load_pokemon
   - db_connection.py abstracts database connectivity db_connection

2. Data Transformation:
- JSON arrays (types, abilities, stats) â†’ normalized mapping tables
- Caches (type_cache, ability_cache) prevent duplicate inserts
- Structured stats dictionary â†’ row-based storage for flexibility

3. Database:
- SQL Server chosen for robustness and real-world relevance
- Schema normalization ensures scalability for thousands of PokÃ©mon

## Assumptions
- PokÃ©mon IDs from the API are globally unique so they are used as primary keys
- Types and abilities are reusable entities. Hence separate lookup tables are created
- Only a subset of PokÃ©mon attributes is stored (height, weight, base exp, stats, abilities, types) for clarity.
- The pipeline is designed to handle duplicates via caches.

## Future Improvements
- Add logging & monitoring for ETL runs
- Parameterize pipeline to fetch all PokÃ©mon instead of a limited subset
- Add Airflow orchestration for scheduled ETL runs
- Build visual dashboard (Power BI / Tableau) for PokÃ©mon analytics

## Conclusion 
This project showcases:
- Data engineering fundamentals â€“ designing ETL pipelines, handling APIs, database normalization.
- Production-ready practices â€“ Dockerization, environment variables, modular scripts.
- SQL + Python integration â€“ strong foundation for data workflows.

## Contributor 
Komal - komal202220@gmail.com
